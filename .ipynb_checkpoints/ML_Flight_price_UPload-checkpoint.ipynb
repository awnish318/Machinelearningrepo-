{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b35b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a46b8b2c",
   "metadata": {},
   "source": [
    "#### importing all the necessary libraries !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bc16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8655a7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_excel(r\"Data_Train.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3513b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### why there is a need to append 'r' (raw_string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1487c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\tPython\n"
     ]
    }
   ],
   "source": [
    "print('Hello\\tPython')\n",
    "\n",
    "### Now, here 'Hello\\tPython' is a normal string literal, the sequences “\\t” will be treated as escape characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b728e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af752aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7de3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b27c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbdf6d",
   "metadata": {},
   "source": [
    "#### Importing dataset\n",
    "    1.Since data is in form of excel file we have to use pandas read_excel to load the data\n",
    "    2.After loading it is important to check null values in a column or a row\n",
    "    3.If it is present then following can be done,\n",
    "        a.Filling NaN values with mean, median and mode using fillna() method\n",
    "        b.If Less missing values, we can drop it as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b055d208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4add6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()\n",
    "\n",
    "## train_data.isnull().sum(axis=0)\n",
    "## by-default axis is 0 , ie it computes total missing values column-wise !\n",
    "\n",
    "## train_data.isnull().sum(axis=1) -->> if axis=1 , ie it computes total missing values row-wise !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda2961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f98b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### getting all the rows where we have missing value\n",
    "train_data[train_data['Total_Stops'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331fc163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fefda914",
   "metadata": {},
   "source": [
    "#### as we have 1 missing value , I can directly drop these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3d49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d5e87d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04342e31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3a9f60a",
   "metadata": {},
   "source": [
    "## Pre-process & Perform Featurization of \"Date_of_Journey\"\n",
    "    ie pre-process it & extract day,month,year from \"Date_of_Journey\" feature.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34648527",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac7f3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e321d17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c47ae740",
   "metadata": {},
   "source": [
    "we can see that Date_of_Journey is a object data type,\n",
    "Therefore, we have to convert this datatype into timestamp bcz our \n",
    "model will not be able to understand these string values,it just understand Time-stamp..\n",
    "\n",
    "For this we require pandas to_datetime to convert object data type to datetime dtype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49c80b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8825899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_into_datetime(col):\n",
    "    data[col]=pd.to_datetime(data[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674605ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['Date_of_Journey','Dep_Time', 'Arrival_Time']:\n",
    "    change_into_datetime(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4119e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883f00d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date_of_Journey'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date_of_Journey'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb375d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "### it means our data belongs to 2019 year only, hence extracting year feature & consider this as a input to my machine learning model makes no sense !\n",
    "### but if we have more than 1 year  , then of-course it may impact !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a76d13c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e7916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d931",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['journey_day']=data['Date_of_Journey'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2953c1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['journey_month']=data['Date_of_Journey'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['journey_year']=data['Date_of_Journey'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d988395",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Date_of_Journey',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fbc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdca6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85d03969",
   "metadata": {},
   "source": [
    "## Lets try to clean Dep_Time & Arrival_Time & featurize it..¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hour_min(df,col):\n",
    "    df[col+'_hour']=df[col].dt.hour\n",
    "    df[col+'_minute']=df[col].dt.minute\n",
    "    df.drop(col,axis=1,inplace=True)\n",
    "    return df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Departure time is when a plane leaves the gate\n",
    "\n",
    "extract_hour_min(data,'Dep_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5bdd11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bedc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets Featurize 'Arrival_Time' !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0819bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_hour_min(data,'Arrival_Time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3292ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea2fde0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b638ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb94bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67776691",
   "metadata": {},
   "source": [
    "## lets analyse when will most of the flights will take-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0413a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Converting the flight Dep_Time into proper time i.e. mid_night, morning, afternoon and evening.\n",
    "\n",
    "def flight_dep_time(x):\n",
    "    '''\n",
    "    This function takes the flight Departure time \n",
    "    and convert into appropriate format.\n",
    "    '''\n",
    "    if ( x> 4) and (x<=8 ):\n",
    "        return 'Early mrng'\n",
    "    \n",
    "    elif ( x>8 ) and (x<=12 ):\n",
    "        return 'Morning'\n",
    "    \n",
    "    elif ( x>12 ) and (x<=16 ):\n",
    "        return 'Noon'\n",
    "    \n",
    "    elif ( x>16 ) and (x<=20 ):\n",
    "        return 'Evening'\n",
    "    \n",
    "    elif ( x>20 ) and (x<=24 ):\n",
    "        return 'Night'\n",
    "    else:\n",
    "        return 'Late night'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Dep_Time_hour'].apply(flight_dep_time).value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed40c400",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad79f23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33b9e3a7",
   "metadata": {},
   "source": [
    "## lets use Cufflinks & plotly to make your visuals more interactive !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7cf3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710519c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets use Plotly interactive plots directly with Pandas dataframes, but First u need below set-up !\n",
    "\n",
    "import plotly\n",
    "import cufflinks as cf\n",
    "from cufflinks.offline import go_offline\n",
    "from plotly.offline import download_plotlyjs,init_notebook_mode,plot,iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e8212",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Dep_Time_hour'].apply(flight_dep_time).value_counts().iplot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e254be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68176c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1aa7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f30a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b267f44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7da831dd",
   "metadata": {},
   "source": [
    "## Pre-process Duration Feature & extract meaningful features "
   ]
  },
  {
   "cell_type": "raw",
   "id": "d008db5e",
   "metadata": {},
   "source": [
    "Lets Apply pre-processing on duration column,\n",
    "-->> Once we pre-processed our Duration feature , lets featurize this feature & extract Duration hours and minute from duration..\n",
    "\n",
    "\n",
    "-->> As my ML model is not able to understand this duration as it contains string values , thats why we have to tell our\n",
    "Ml Model that this is Duration_hour & this Duration_is minute.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613c4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_duration(x):\n",
    "    if 'h' not in x:\n",
    "        x='0h '+x\n",
    "    elif 'm' not in x:\n",
    "        x=x+' 0m'\n",
    "    return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c54d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Duration']=data['Duration'].apply(preprocess_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a22fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Duration']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a740019",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Duration'][0].split(' ')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d7009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(data['Duration'][0].split(' ')[0][0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d643e",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(data['Duration'][0].split(' ')[1][0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c5fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Duration_hours']=data['Duration'].apply(lambda x:int(x.split(' ')[0][0:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3ab960",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Duration_mins']=data['Duration'].apply(lambda x:int(x.split(' ')[1][0:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a56d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b01fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063d644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "201c1efb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2ac9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae586c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Lets Analyse whether Duration impacts on Price or not ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec8d133",
   "metadata": {},
   "outputs": [],
   "source": [
    "'2*60+50*1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e174e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### eval is a in-built function of python which evaluates the “String” like a python expression and returns the result as an integer.\n",
    "eval('2*60+50*1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e17b67b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Duration_total_mins']=data['Duration'].str.replace('h','*60').str.replace(' ','+').str.replace('m','*1').apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8085c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38578903",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5aec1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### It Plot data and regression model fits across a FacetGrid.. (combination of 'regplot` and :class:`FacetGrid)\n",
    "#### its a extended form of scatter plot..\n",
    "\n",
    "sns.lmplot(x='Duration_total_mins',y='Price',data=data)\n",
    "\n",
    "\n",
    "## Conclusion-->> pretty clear that As the duration of minutes increases Flight price also increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb6fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf033d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4171ab52",
   "metadata": {},
   "source": [
    "## which city has maximum final destination of flights ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb6fc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Destination'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31277c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Destination'].value_counts().plot(kind='pie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3e551",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Inference->> \n",
    "Final destination of majority of flights is Cochin. There are two values for Delhi destination which needs to be corrected,\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda6268",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98c5f454",
   "metadata": {},
   "source": [
    "## Lets Perform Exploratory Data Analysis(Bivariate Analysis) to come up with some business insights\n",
    "    Problem Statement-->> on which route Jet Airways is extremely used???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3295092",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Route']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bbffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Airline']=='Jet Airways'].groupby('Route').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5d36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "673fe808",
   "metadata": {},
   "source": [
    "## Airline vs Price Analysis\n",
    "    ie finding price distribution & 5-point summary of each Airline.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b531f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.boxplot(y='Price',x='Airline',data=data)\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf8fed04",
   "metadata": {},
   "source": [
    "Conclusion--> From graph we can see that Jet Airways Business have the highest Price., Apart from the first Airline almost all are having similar median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06122d15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec848c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "### when we need boxplot + distribution both , its good to consider violinplot.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.violinplot(y='Price',x='Airline',data=data)\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ce779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48288ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56714455",
   "metadata": {},
   "source": [
    "## Lets Perform Feature-Encoding on Data !\n",
    "    Applying one-hot on data !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f9b468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392df0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## but lets remove some of the un-necessary features !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a21c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ebc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(data['Additional_Info'].value_counts()/len(data)*100,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1aad58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional_Info contains almost 80% no_info,so we can drop this column\n",
    "# we can drop Route as well as we have pre-process that column\n",
    "## lets drop Duration_total_mins as we have already extracted \"Duration_hours\" & \"Duration_mins\"\n",
    "\n",
    "data.drop(columns=['Additional_Info','Route','Duration_total_mins','journey_year'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68906bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007279b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737e1e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9277f93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8203480",
   "metadata": {},
   "source": [
    "## Lets separate categorical data & numerical data !\n",
    "    categorical data are those whose data-type is 'object'\n",
    "    Numerical data are those whose data-type is either int of float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9623cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col=[col for col in data.columns if data[col].dtype=='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d346e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_col=[col for col in data.columns if data[col].dtype!='object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa183b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b397b79d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450d154d",
   "metadata": {},
   "source": [
    "## Handling Categorical Data\n",
    "    We are using 2 basic Encoding Techniques to convert Categorical data into some numerical format\n",
    "    if data belongs to Nominal data (ie data is not in any order) -->> OneHotEncoder is used in this case\n",
    "    if data belongs to Ordinal data (ie data is in order ) -->>       LabelEncoder is used in this case\n",
    "\n",
    "    But in real-world , it is not necessary that u have to always One-hot or label , hence we will discuss more interesting approaches to do this !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a70872b",
   "metadata": {},
   "source": [
    "### Lets apply one-hot encoding on 'Source' feature !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eafc90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-hot from scratch :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Source'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2f1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d557d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Source'].apply(lambda x: 1 if x=='Banglore' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f912fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in data['Source'].unique():\n",
    "    data['Source_'+category]=data['Source'].apply(lambda x: 1 if x==category else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5c28c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad129020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdd19969",
   "metadata": {},
   "source": [
    "## Performing Target Guided Mean Encoding !\n",
    "    ofcourse we can use One-hot , but if we have more sub-categories , it creates curse of dimensionality in ML..\n",
    "    lets use Target Guided Mean Encoding in order to get rid of this.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64095ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines=data.groupby(['Airline'])['Price'].mean().sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566764f",
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df525826",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={key:index for index,key in enumerate(airlines,0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2495bb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e756e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Airline']=data['Airline'].map(dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204d42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Airline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478b7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1bcfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f51e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Destination'].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b0a2780",
   "metadata": {},
   "source": [
    "Note: till now , Delhi (Capital of India) has one Airport & its second Airport is yet to build in Greater Noida (Jewar)\n",
    "      which is part of NCR , so we will consider New Delhi & Delhi as same ...\n",
    "\n",
    "\n",
    "      but in future , these conditions may change.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d6032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Destination'].replace('New Delhi','Delhi',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d2daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Destination'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4bedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest=data.groupby(['Destination'])['Price'].mean().sort_values().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e57fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2={key:index for index,key in enumerate(dest,0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e8dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbe479",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Destination']=data['Destination'].map(dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421332ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Destination']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32e3ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f30cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "828c5a2d",
   "metadata": {},
   "source": [
    "### Perform Manual Encoding on Total_stops feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_Stops'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f8473",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops={'non-stop':0, '2 stops':2, '1 stop':1, '3 stops':3, '4 stops':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878e20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_Stops']=data['Total_Stops'].map(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf4ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Total_Stops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d2b078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78c31588",
   "metadata": {},
   "source": [
    "## Performing Outlier Detection !\n",
    "    Here the list of data visualization plots to spot the outliers.\n",
    "1. Box and whisker plot (box plot).\n",
    "2. Scatter plot.\n",
    "3. Histogram.\n",
    "4. Distribution Plot.\n",
    "5. QQ plot"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d821ee89",
   "metadata": {},
   "source": [
    "CAUSE FOR OUTLIERS\n",
    "* Data Entry Errors:- Human errors such as errors caused during data collection, recording, or entry can cause outliers in data.\n",
    "* Measurement Error:- It is the most common source of outliers. This is caused when the measurement instrument used turns out to be faulty.\n",
    "* Natural Outlier:- When an outlier is not artificial (due to error), it is a natural outlier. Most of real world data belong to this category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42563a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df,col):\n",
    "    fig,(ax1,ax2,ax3)=plt.subplots(3,1)\n",
    "    sns.distplot(df[col],ax=ax1)\n",
    "    sns.boxplot(df[col],ax=ax2)\n",
    "    sns.distplot(df[col],ax=ax3,kde=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3762b075",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data,'Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73e95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d023b303",
   "metadata": {},
   "source": [
    "### getting a high level over-view of various ways to deal with outliers:\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6eeab423",
   "metadata": {},
   "source": [
    "\n",
    "Again there are various ways to deal with outliers :\n",
    "\n",
    "\n",
    "1..Statistical imputation , ie impute it with mean , median or mode of data..\n",
    "\n",
    "a..Whenever ur data is Gaussian Distributed ,use 3 std dev approach to remove outliers in such case\n",
    "     ie we will use u+3*sigma & u-3*sigma\n",
    "        data pts greater than upper_boundary( u+3*sigma) are my outliers \n",
    "            & data pts which are less than lower_boundary(u-3*sigma) are my outliers\n",
    "\n",
    "        Above approach is known as Z-score & it has a extended version known as Robust z-score..\n",
    "        Robust Z-score is also called as Median absolute deviation method. \n",
    "        It is similar to Z-score method with some changes in parameters.\n",
    "\n",
    "\n",
    "b..If Features Are Skewed We Use the below Technique which is IQR\n",
    "    Data which are greater than IQR +1.5 IQR and data which are below than IQR - 1.5 IQR are my outliers\n",
    "     where IQR=75th%ile data - 25th%ile data\n",
    "\n",
    "     & IQR +- 1.5 IQR  will be changed depending upon the domain ie it may be IQR + 3IQR \n",
    "\n",
    "\n",
    "       Extended version of above is WINSORIZATION METHOD(PERCENTILE CAPPING)..\n",
    "       This method is similar to IQR method. It says -->> \n",
    "\n",
    "       Data points that are greater than 99th percentile and data points that are below tha 1st percentile \n",
    "       are treated as outliers.\n",
    "\n",
    "\n",
    "\n",
    " c..If we have huge high dimensional data , then it is good to perform isolation forest...\n",
    "     It is a clustering algo which works based on decision tree and it isolate the outliers.\n",
    "     It classify the data point to outlier and not outliers..\n",
    "         If the result is -1, it means that this specific data point is an outlier. \n",
    "         If the result is 1, then it means that the data point is not an outlier.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "So we have tonnes of ways to deal with outliers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6c527f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Price']=np.where(data['Price']>=35000,data['Price'].median(),data['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf46cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(data,'Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85705896",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb40176",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['Source','Duration'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7394b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49939215",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefc15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29058203",
   "metadata": {},
   "source": [
    "## Performing Feature Selection !"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad8db4ff",
   "metadata": {},
   "source": [
    "\n",
    "Finding out the best feature which will contribute most to the target variable. \n",
    "Lets get a high level overview of most of the frequently used feature selection technique..\n",
    "\n",
    "\n",
    "Why to apply Feature Selection?\n",
    "To select important features to get rid of curse of dimensionality ie..to get rid of duplicate features\n",
    "\n",
    "\n",
    "ways or technqiues to do it if we have regression use-case\n",
    "a..SelectKBest\n",
    "    Score function:\n",
    "    \n",
    "    For regression: f_regression, mutual_info_regression\n",
    "\n",
    "    f_regression\n",
    "    Its backbone is pearson co-relation.. \n",
    "\n",
    "\n",
    "    mutual_info_regression \n",
    "    Its Backbone is Various statistical test like Chi-sq,Anova & p-value.\n",
    "\n",
    "\n",
    "b..ExtraTreesClassifier\n",
    "   This technique gives you a score for each feature of your data,the higher the score more relevant it is\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22425bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bf6b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=data.drop(['Price'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c990a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=data['Price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d806a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32995d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_regression(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821813f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp=pd.DataFrame(mutual_info_regression(X,y),index=X.columns)\n",
    "imp.columns=['importance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3723e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.sort_values(by='importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3324378",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93b5ad75",
   "metadata": {},
   "source": [
    "## Lets build ML Model  , then later on we can think of saving it.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52abb121",
   "metadata": {},
   "source": [
    "#### split dataset into train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43368422",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de82e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b472c",
   "metadata": {},
   "source": [
    "#### what we often do in modelling:\n",
    "    a..Initially ,lets build basic random forest model.\n",
    "    b..then later-on , we will try to improve this model using some parameters..\n",
    "    c..Then we will hyper-tune my model to get optimal value of parameters in order to achieve optimal value of params.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6320f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_model=RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0bc9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ml_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f22bba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287d43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55a7d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "667416b5",
   "metadata": {},
   "source": [
    "### How to save ML model into disk"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ba6ba8cc",
   "metadata": {},
   "source": [
    "lets try to dump ml model using pickle & joblib..\n",
    "advantage of dumping--\n",
    "imagine in future we have new data ,& lets say we have to predict price on this huge data\n",
    "\n",
    "then just for this new data , we have to execute all the above cells follow the entire pipeline,  then only we are able to predict on this...\n",
    "\n",
    "\n",
    "so to get rid of such issue , will just dump it to reuse it again & again..\n",
    "what does this file store??\n",
    "this save coefficients of our model.. not an entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517e7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### !pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb7faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17bfa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(r'E:\\End-2-end Projects\\Flight_Price\\Datasets/rf_random.pkl','wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d668611b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade9047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=open(r'E:\\End-2-end Projects\\Flight_Price\\Datasets/rf_random.pkl','rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0df3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest=pickle.load(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b16596",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068e8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0df855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8886524a",
   "metadata": {},
   "source": [
    "## Defining your own evaluation metric :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a394304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true,y_pred):\n",
    "    y_true,y_pred=np.array(y_true),np.array(y_pred)\n",
    "    \n",
    "    return np.mean(np.abs((y_true-y_pred)/y_true))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070ae9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mape(y_test,forest.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dc96d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "372afb7a",
   "metadata": {},
   "source": [
    "## How to Automate ML Pipeline :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927fbeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(ml_model):\n",
    "    \n",
    "    model=ml_model.fit(X_train,y_train)\n",
    "    print('Training_score: {}'.format(model.score(X_train,y_train)))\n",
    "    y_prediction=model.predict(X_test)\n",
    "    print('Predictions are : {}'.format(y_prediction))\n",
    "    print('\\n')\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    r2_score=metrics.r2_score(y_test,y_prediction)\n",
    "    print('r2_score: {}'.format(r2_score))\n",
    "    print('MSE : ', metrics.mean_squared_error(y_test,y_prediction))\n",
    "    print('MAE : ', metrics.mean_absolute_error(y_test,y_prediction))\n",
    "    print('RMSE : ', np.sqrt(metrics.mean_squared_error(y_test,y_prediction)))\n",
    "    print('MAPE : ', mape(y_test,y_prediction))\n",
    "    sns.distplot(y_test-y_prediction)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ff6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(RandomForestRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5da9be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d261c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f9e17750",
   "metadata": {},
   "source": [
    "## how to hypertune ml model\n",
    "    Hyperparameter Tuning or Hyperparameter Optimization\n",
    "    1.Choose following method for hyperparameter tuning\n",
    "        a.RandomizedSearchCV --> Fast way to Hypertune model\n",
    "        b.GridSearchCV--> Slow way to hypertune my model\n",
    "    2.Choose ML algo that u have to hypertune\n",
    "    2.Assign hyperparameters in form of dictionary or create hyper-parameter space\n",
    "    3.define searching &  apply searching on Training data or  Fit the CV model \n",
    "    4.Check best parameters and best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781ae5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "### initialise your estimator\n",
    "reg_rf=RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(start=1000,stop=1200,num=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f62eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators=[int(x) for x in np.linspace(start=1000,stop=1200,num=6)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features=[\"auto\", \"sqrt\"]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth=[int(x) for x in np.linspace(start=5,stop=30,num=4)]\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split=[5,10,15,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcd67f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid or hyper-parameter space\n",
    "random_grid={\n",
    "    'n_estimators':n_estimators,\n",
    "    'max_features':max_features,\n",
    "    'max_depth':max_depth,\n",
    "    'min_samples_split':min_samples_split\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6e6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5c803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_Random=RandomizedSearchCV(reg_rf,param_distributions=random_grid,cv=3,verbose=2,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4548bfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_Random.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aebd4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### to get your best model..\n",
    "rf_Random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f67fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2=rf_Random.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.r2_score(y_test,pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffafab2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f1bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd9d3c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8216ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bac112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
